# TinyLM

Implementation of Language Models from Scratch for Self-Educational Purposes

## About

TinyLM is a set of language model implementation from scratch for self-educational purposes, and focuses on:

- Training language models from scratch using JAX/Flax
  - [ ] Auto-regressive language model training
  - [ ] Various post-training techniques
    - [ ] Supervised fine tuning (SFT)
    - [ ] Low-rank adaptation (LoRA)
    - [ ] Proximal policy optimization (PPO)
    - [ ] Direct preference optimization (DPO)
    - [ ] Group relative policy optimization (GRPO)
  - [ ] Multi-token prediction training
- Modern language model inference architectures
  - [ ] Key-value caching with paging
  - [ ] Just-in-Time optimized execution
  - [ ] Dynamic batching
